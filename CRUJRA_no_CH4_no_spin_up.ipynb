{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fb8edaa",
   "metadata": {},
   "source": [
    "# CRUJRA  (no_CH4_no_spin_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9906f283",
   "metadata": {},
   "source": [
    "## 1.Make the nml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03e7bdd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import subprocess\n",
    "import glob, os, shutil, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import time\n",
    "\n",
    "def make_namelist(nml_input,nml_output,station_list,mode='no_spin_up',forcing='FLUXNET-CH4'):\n",
    "    # open nml file and readlines\n",
    "    with open(nml_input+f'US-Los_{forcing}_no_CH4.nml', 'r') as file:\n",
    "        nml_content = file.readlines()\n",
    "\n",
    "    # modify var\n",
    "    for i, line in enumerate(nml_content):\n",
    "        # replace CASE NAME\n",
    "        if 'DEF_CASE_NAME' in line:\n",
    "            nml_content[i] = f\"DEF_CASE_NAME = '{station_list['SITE_ID']}'\\n\"\n",
    "        if 'DEF_simulation_time%start_year' in line:\n",
    "            nml_content[i] = f\"DEF_simulation_time%start_year = {int(station_list['YEAR_START'])}\\n\"\n",
    "        if 'DEF_simulation_time%end_year' in line:\n",
    "            nml_content[i] = f\"DEF_simulation_time%end_year = {int(station_list['YEAR_END'])}\\n\"\n",
    "        if 'DEF_simulation_time%spinup_year' in line:\n",
    "            nml_content[i] = f\"DEF_simulation_time%spinup_year = {int(station_list['YEAR_START'])-1}\\n\"\n",
    "        if 'DEF_simulation_time%spinup_month' in line:\n",
    "            nml_content[i] = f\"DEF_simulation_time%spinup_month = 1\\n\"\n",
    "        if 'DEF_simulation_time%spinup_day' in line:\n",
    "            nml_content[i] = f\"DEF_simulation_time%spinup_day = 1\\n\"\n",
    "        if 'DEF_simulation_time%spinup_sec' in line:\n",
    "            nml_content[i] = f\"DEF_simulation_time%spinup_sec = 0\\n\"\n",
    "        if 'DEF_simulation_time%spinup_repeat' in line:\n",
    "            nml_content[i] = f\"DEF_simulation_time%spinup_repeat = 0\\n\"\n",
    "        if 'SITE_fsitedata' in line:\n",
    "            nml_content[i] = f\"SITE_fsitedata = '{station_list['srfpath']}'\\n\"\n",
    "        if 'DEF_dir_output' in line:\n",
    "            nml_content[i] = f\"DEF_dir_output = '/share/home/dq076/data/cases/site/{forcing}/{mode}/'\\n\"\n",
    "        # if 'DEF_forcing_namelist' in line:\n",
    "        #     nml_content[i] = f\"DEF_forcing_namelist = '{nml_input}/cases/no_spin_up/forcing/SINGLE_{station_list['SITE_ID']}.nml'\\n\"\n",
    "\n",
    "    # read back modified nml file\n",
    "    # sometimes need modify nml file path\n",
    "\n",
    "    new_file_path = f\"{nml_output}{station_list['SITE_ID']}.nml\"\n",
    "    with open(new_file_path, 'w') as file:\n",
    "        file.writelines(nml_content)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mode='no_CH4_no_spin_up'\n",
    "    forcing = 'CRUJRA'\n",
    "\n",
    "    nml_input = \"/share/home/dq076/mode/ME/CoLM202X_CH4_s/run/\"\n",
    "    nml_output = f\"{nml_input}site/{forcing}/{mode}/\"\n",
    "    os.makedirs(nml_output, exist_ok=True)\n",
    "\n",
    "    stnlist = f\"/share/home/dq076/data/ME/FLUXNET-CH4/FLX_AA-Flx_CH4-META_20201112135337801132.csv\"\n",
    "    station_lists = pd.read_csv(stnlist, header=0)\n",
    "    n = len(station_lists['SITE_ID'])\n",
    "    for i in range(n):\n",
    "        station_list = station_lists.iloc[i]\n",
    "        station_list['srfpath'] = f'/share/home/dq076/data/CoLM_Forcing/PLUMBER2/Srfdata/{station_list['SITE_ID']}_{str(station_list['YEAR_START'])}-{str(station_list['YEAR_END'])}_FLUXNET-CH4_Srf.nc'     \n",
    "        make_namelist(nml_input,nml_output,station_list,mode,forcing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfa3d6a",
   "metadata": {},
   "source": [
    "## 2.Run the CoLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969df930",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "import glob\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed  # 切换到 ProcessPoolExecutor 以支持实时进度\n",
    "\n",
    "def load_environment(env_file):\n",
    "    cmd = f'bash -c \"source {env_file} && env\"'\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    \n",
    "    new_env = {}\n",
    "    for line in result.stdout.strip().split('\\n'):\n",
    "        if '=' in line:\n",
    "            key, value = line.split('=', 1)\n",
    "            if not key.startswith('BASH_FUNC_'):\n",
    "                new_env[key] = value\n",
    "    \n",
    "    os.environ.update(new_env)\n",
    "    return os.environ.copy()\n",
    "\n",
    "def run_colm(run_path, nml_path, log_path, nml_name, updated_env):\n",
    "    \"\"\"\n",
    "    处理单个 nml 文件，返回成功/失败状态（不打印进度，由主脚本处理）。\n",
    "    \"\"\"\n",
    "    nml_file = f'{nml_path}{nml_name}.nml'\n",
    "    log_file = f'{log_path}{nml_name}.txt'\n",
    "    \n",
    "    try:\n",
    "        # 用 'w' 模式打开文件，重置内容\n",
    "        with open(log_file, 'w', encoding='utf-8') as log:\n",
    "            log.write(f\"=== 处理 {nml_name}.nml ===\\n\")\n",
    "            log.flush()\n",
    "            \n",
    "            commands = [\n",
    "                [f'{run_path}mksrfdata.x', nml_file],\n",
    "                [f'{run_path}mkinidata.x', nml_file],\n",
    "                [f'{run_path}colm.x', nml_file]\n",
    "            ]\n",
    "            \n",
    "            for cmd in commands:\n",
    "                log.write(f\"执行命令: {' '.join(cmd)}\\n\")\n",
    "                log.flush()\n",
    "                \n",
    "                subprocess.run(cmd, \n",
    "                               env=updated_env, \n",
    "                               stdout=log, \n",
    "                               stderr=subprocess.STDOUT, \n",
    "                               text=True)\n",
    "                \n",
    "                log.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "                log.flush()\n",
    "            \n",
    "            log.write(f\"=== {nml_name} 处理完成 ===\\n\")\n",
    "            log.flush()\n",
    "        \n",
    "        return True  # 成功\n",
    "    except Exception as e:\n",
    "        # 如果失败，也记录到日志\n",
    "        with open(log_file, 'w', encoding='utf-8') as log:\n",
    "            log.write(f\"=== {nml_name}.nml 处理失败: {str(e)} ===\\n\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    forcing ='CRUJRA'\n",
    "    mode ='no_CH4_no_spin_up'\n",
    "\n",
    "    env_file = '/share/home/dq089/soft/gnu-env'\n",
    "    run_path = '/share/home/dq076/mode/ME/CoLM202X_CH4_s/run/'\n",
    "    nml_path = f'{run_path}site/{forcing}/{mode}/'\n",
    "    log_path = f'{nml_path}logs/'  \n",
    "    os.makedirs(log_path, exist_ok=True)\n",
    "\n",
    "    updated_env = load_environment(env_file)\n",
    "    nml_files = glob.glob(f'{nml_path}*.nml')\n",
    "    nml_names = [os.path.splitext(os.path.basename(nml_file))[0] for nml_file in nml_files]\n",
    "    print(f\"发现 {len(nml_files)} 个 .nml 文件：{nml_names}\")\n",
    "    \n",
    "    # 切换到 ProcessPoolExecutor 以支持 as_completed 实时进度\n",
    "    max_workers = min(24, os.cpu_count() or 1)\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # 提交所有任务，返回 future 对象\n",
    "        future_to_nml = {\n",
    "            executor.submit(run_colm, run_path, nml_path, log_path, nml_name, updated_env): nml_name\n",
    "            for nml_name in nml_names\n",
    "        }\n",
    "        \n",
    "        # 维护剩余任务集合\n",
    "        remaining_nml = set(nml_names)\n",
    "        completed_count = 0\n",
    "        \n",
    "        # 实时监控完成\n",
    "        for future in as_completed(future_to_nml):\n",
    "            nml_name = future_to_nml[future]\n",
    "            try:\n",
    "                success = future.result()\n",
    "                if success:\n",
    "                    completed_count += 1\n",
    "                remaining_nml.discard(nml_name)  # 移除已完成（无论成功/失败）\n",
    "                \n",
    "                # 打印进度：总数 + 剩余列表\n",
    "                print(f\"=== {nml_name} 处理完成（成功: {success}) ===\")\n",
    "                print(f\"已完成总数: {completed_count}/{len(nml_names)}\")\n",
    "                if remaining_nml:\n",
    "                    print(f\"剩余未处理: {sorted(list(remaining_nml))}\")\n",
    "                else:\n",
    "                    print(\"所有任务已完成！\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "            except Exception as exc:\n",
    "                print(f\"{nml_name} 执行异常: {exc}\")\n",
    "                remaining_nml.discard(nml_name)\n",
    "                completed_count += 1  # 视作完成（失败）\n",
    "                print(f\"已完成总数: {completed_count}/{len(nml_names)}\")\n",
    "                if remaining_nml:\n",
    "                    print(f\"剩余未处理: {sorted(list(remaining_nml))}\")\n",
    "                print(\"-\" * 50)\n",
    "    \n",
    "    print(\"批量处理结束。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cb45ec",
   "metadata": {},
   "source": [
    "## 3.Postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a7985f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "\n",
    "def merge(i,station_lists,data_path):\n",
    "    station_list = station_lists.iloc[i]\n",
    "    case = station_list['SITE_ID']\n",
    "    yrstt = station_list['YEAR_START']\n",
    "    yrend = station_list['YEAR_END']\n",
    "    history_path = f'{data_path}/{case}/history/'\n",
    "    postdata_path = f'{data_path}/{case}/postdata/'\n",
    "    postdata_name = f'{case}_hist_{yrstt}-{yrend}.nc'\n",
    "    os.makedirs(postdata_path,exist_ok=True)\n",
    "\n",
    "    nc_files = [f for f in os.listdir(history_path) if f.endswith('.nc')]\n",
    "    if nc_files:\n",
    "        if yrstt==yrend:\n",
    "            os.system(f'cp {history_path}{case}_hist_{yrstt}.nc {postdata_path}{postdata_name}')\n",
    "        else:\n",
    "            os.system(f'cdo -O -mergetime {history_path}*.nc {postdata_path}{postdata_name}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    forcing = 'CRUJRA'\n",
    "    mode = 'no_CH4_no_spin_up'\n",
    "\n",
    "    cases_path = '/share/home/dq076/data/cases/site/'\n",
    "    data_path = f'{cases_path}/{forcing}/{mode}/'\n",
    "\n",
    "    stnlist = f\"/share/home/dq076/data/ME/FLUXNET-CH4/FLX_AA-Flx_CH4-META_20201112135337801132.csv\"\n",
    "    station_lists = pd.read_csv(stnlist, header=0)\n",
    "    station_lists = station_lists[station_lists['FLUXNET-CH4_DATA_POLICY'] == 'CCBY4.0'].reset_index(drop=True)\n",
    "    \n",
    "    results = Parallel(n_jobs=24)(delayed(merge)(i,station_lists,data_path) for i in range(station_lists.shape[0]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
