{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fb8edaa",
   "metadata": {},
   "source": [
    "# CRUJRA  (spin_up_80yr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9906f283",
   "metadata": {},
   "source": [
    "## 1.Make the nml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03e7bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import subprocess\n",
    "import glob, os, shutil, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import time\n",
    "\n",
    "def make_namelist(nml_input,nml_output,station_list,mode='no_spin_up',forcing='FLUXNET-CH4'):\n",
    "    # open nml file and readlines\n",
    "    with open(nml_input+f'US-Los_{forcing}.nml', 'r') as file:\n",
    "        nml_content = file.readlines()\n",
    "\n",
    "    # modify var\n",
    "    for i, line in enumerate(nml_content):\n",
    "        # replace CASE NAME\n",
    "        if 'DEF_CASE_NAME' in line:\n",
    "            nml_content[i] = f\"DEF_CASE_NAME = '{station_list['SITE_ID']}'\\n\"\n",
    "        if 'DEF_simulation_time%start_year' in line:\n",
    "            nml_content[i] = f\"DEF_simulation_time%start_year = {int(station_list['YEAR_START']-80)}\\n\"\n",
    "        if 'DEF_simulation_time%end_year' in line:\n",
    "            nml_content[i] = f\"DEF_simulation_time%end_year = {int(station_list['YEAR_END'])}\\n\"\n",
    "        if 'DEF_simulation_time%spinup_year' in line:\n",
    "            nml_content[i] = f\"DEF_simulation_time%spinup_year = {int(station_list['YEAR_START'])}\\n\"\n",
    "        if 'DEF_simulation_time%spinup_month' in line:\n",
    "            nml_content[i] = f\"DEF_simulation_time%spinup_month = 1\\n\"\n",
    "        if 'DEF_simulation_time%spinup_day' in line:\n",
    "            nml_content[i] = f\"DEF_simulation_time%spinup_day = 1\\n\"\n",
    "        if 'DEF_simulation_time%spinup_sec' in line:\n",
    "            nml_content[i] = f\"DEF_simulation_time%spinup_sec = 0\\n\"\n",
    "        if 'DEF_simulation_time%spinup_repeat' in line:\n",
    "            nml_content[i] = f\"DEF_simulation_time%spinup_repeat = 0\\n\"\n",
    "        if 'SITE_fsitedata' in line:\n",
    "            nml_content[i] = f\"SITE_fsitedata = '{station_list['srfpath']}'\\n\"\n",
    "        if 'DEF_dir_output' in line:\n",
    "            nml_content[i] = f\"DEF_dir_output = '/share/home/dq076/data/cases/site/{forcing}/{mode}/'\\n\"\n",
    "        # if 'DEF_forcing_namelist' in line:\n",
    "        #     nml_content[i] = f\"DEF_forcing_namelist = '{nml_input}/cases/no_spin_up/forcing/SINGLE_{station_list['SITE_ID']}.nml'\\n\"\n",
    "\n",
    "    # read back modified nml file\n",
    "    # sometimes need modify nml file path\n",
    "\n",
    "    new_file_path = f\"{nml_output}{station_list['SITE_ID']}.nml\"\n",
    "    with open(new_file_path, 'w') as file:\n",
    "        file.writelines(nml_content)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    stnlist = f\"/share/home/dq076/data/ME/FLUXNET-CH4/FLX_AA-Flx_CH4-META_20201112135337801132.csv\"\n",
    "    station_lists = pd.read_csv(stnlist, header=0)\n",
    "    n = len(station_lists['SITE_ID'])\n",
    "    mode='spin_up_80yr'\n",
    "    forcing = 'CRUJRA'\n",
    "    nml_input = \"/share/home/dq076/mode/ME/CoLM202X_CH4_s/run/\"\n",
    "    nml_output = f\"{nml_input}site/{forcing}/{mode}/\"\n",
    "    os.makedirs(nml_output, exist_ok=True)\n",
    "    for i in range(n):\n",
    "        station_list = station_lists.iloc[i]\n",
    "        station_list['srfpath'] = f'/share/home/dq076/data/CoLM_Forcing/PLUMBER2/Srfdata/{station_list['SITE_ID']}_{str(station_list['YEAR_START'])}-{str(station_list['YEAR_END'])}_FLUXNET-CH4_Srf.nc'     \n",
    "        make_namelist(nml_input,nml_output,station_list,mode,forcing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfa3d6a",
   "metadata": {},
   "source": [
    "## 2.Run the CoLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969df930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "import glob\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed  # 切换到 ProcessPoolExecutor 以支持实时进度\n",
    "\n",
    "def load_environment(env_file):\n",
    "    cmd = f'bash -c \"source {env_file} && env\"'\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    \n",
    "    new_env = {}\n",
    "    for line in result.stdout.strip().split('\\n'):\n",
    "        if '=' in line:\n",
    "            key, value = line.split('=', 1)\n",
    "            if not key.startswith('BASH_FUNC_'):\n",
    "                new_env[key] = value\n",
    "    \n",
    "    os.environ.update(new_env)\n",
    "    return os.environ.copy()\n",
    "\n",
    "def run_colm(run_path, nml_path, log_path, nml_name, updated_env):\n",
    "    \"\"\"\n",
    "    处理单个 nml 文件，返回成功/失败状态（不打印进度，由主脚本处理）。\n",
    "    \"\"\"\n",
    "    nml_file = f'{nml_path}{nml_name}.nml'\n",
    "    log_file = f'{log_path}{nml_name}.txt'\n",
    "    \n",
    "    try:\n",
    "        # 用 'w' 模式打开文件，重置内容\n",
    "        with open(log_file, 'w', encoding='utf-8') as log:\n",
    "            log.write(f\"=== 处理 {nml_name}.nml ===\\n\")\n",
    "            log.flush()\n",
    "            \n",
    "            commands = [\n",
    "                [f'{run_path}mksrfdata.x', nml_file],\n",
    "                [f'{run_path}mkinidata.x', nml_file],\n",
    "                [f'{run_path}colm.x', nml_file]\n",
    "            ]\n",
    "            \n",
    "            for cmd in commands:\n",
    "                log.write(f\"执行命令: {' '.join(cmd)}\\n\")\n",
    "                log.flush()\n",
    "                \n",
    "                subprocess.run(cmd, \n",
    "                               env=updated_env, \n",
    "                               stdout=log, \n",
    "                               stderr=subprocess.STDOUT, \n",
    "                               text=True)\n",
    "                \n",
    "                log.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "                log.flush()\n",
    "            \n",
    "            log.write(f\"=== {nml_name} 处理完成 ===\\n\")\n",
    "            log.flush()\n",
    "        \n",
    "        return True  # 成功\n",
    "    except Exception as e:\n",
    "        # 如果失败，也记录到日志\n",
    "        with open(log_file, 'w', encoding='utf-8') as log:\n",
    "            log.write(f\"=== {nml_name}.nml 处理失败: {str(e)} ===\\n\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_file = '/share/home/dq089/soft/gnu-env'\n",
    "    run_path = '/share/home/dq076/mode/ME/CoLM202X_CH4_s/run/'\n",
    "    forcing ='CRUJRA'\n",
    "    mode ='spin_up_80yr'\n",
    "    nml_path = f'{run_path}cases/{forcing}/{mode}/'\n",
    "    log_path = f'{nml_path}logs/'  \n",
    "    os.makedirs(log_path, exist_ok=True)\n",
    "\n",
    "    updated_env = load_environment(env_file)\n",
    "    nml_files = glob.glob(f'{nml_path}*.nml')\n",
    "    nml_names = [os.path.splitext(os.path.basename(nml_file))[0] for nml_file in nml_files]\n",
    "    print(f\"发现 {len(nml_files)} 个 .nml 文件：{nml_names}\")\n",
    "    \n",
    "    # 切换到 ProcessPoolExecutor 以支持 as_completed 实时进度\n",
    "    max_workers = min(24, os.cpu_count() or 1)\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # 提交所有任务，返回 future 对象\n",
    "        future_to_nml = {\n",
    "            executor.submit(run_colm, run_path, nml_path, log_path, nml_name, updated_env): nml_name\n",
    "            for nml_name in nml_names\n",
    "        }\n",
    "        \n",
    "        # 维护剩余任务集合\n",
    "        remaining_nml = set(nml_names)\n",
    "        completed_count = 0\n",
    "        \n",
    "        # 实时监控完成\n",
    "        for future in as_completed(future_to_nml):\n",
    "            nml_name = future_to_nml[future]\n",
    "            try:\n",
    "                success = future.result()\n",
    "                if success:\n",
    "                    completed_count += 1\n",
    "                remaining_nml.discard(nml_name)  # 移除已完成（无论成功/失败）\n",
    "                \n",
    "                # 打印进度：总数 + 剩余列表\n",
    "                print(f\"=== {nml_name} 处理完成（成功: {success}) ===\")\n",
    "                print(f\"已完成总数: {completed_count}/{len(nml_names)}\")\n",
    "                if remaining_nml:\n",
    "                    print(f\"剩余未处理: {sorted(list(remaining_nml))}\")\n",
    "                else:\n",
    "                    print(\"所有任务已完成！\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "            except Exception as exc:\n",
    "                print(f\"{nml_name} 执行异常: {exc}\")\n",
    "                remaining_nml.discard(nml_name)\n",
    "                completed_count += 1  # 视作完成（失败）\n",
    "                print(f\"已完成总数: {completed_count}/{len(nml_names)}\")\n",
    "                if remaining_nml:\n",
    "                    print(f\"剩余未处理: {sorted(list(remaining_nml))}\")\n",
    "                print(\"-\" * 50)\n",
    "    \n",
    "    print(\"批量处理结束。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cb45ec",
   "metadata": {},
   "source": [
    "## 3.Postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a7985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "\n",
    "def merge(i,station_lists,data_path):\n",
    "    station_list = station_lists.iloc[i]\n",
    "    case = station_list['SITE_ID']\n",
    "    yrstt = station_list['YEAR_START']\n",
    "    yrend = station_list['YEAR_END']\n",
    "    history_path = f'{data_path}/{case}/history/'\n",
    "    postdata_path = f'{data_path}/{case}/postdata/'\n",
    "    postdata_name = f'{case}_hist_{yrstt}-{yrend}.nc'\n",
    "    os.makedirs(postdata_path,exist_ok=True)\n",
    "\n",
    "    nc_files = [f for f in os.listdir(history_path) if f.endswith('.nc')]\n",
    "    if nc_files:\n",
    "        if yrstt==yrend:\n",
    "            os.system(f'cp {history_path}{case}_hist_{yrstt}.nc {postdata_path}{postdata_name}')\n",
    "        else:\n",
    "            os.system(f'cdo -O -mergetime {history_path}*.nc {postdata_path}{postdata_name}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    forcing = 'CRUJRA'\n",
    "    mode = 'spin_up_80yr'\n",
    "\n",
    "    cases_path = '/share/home/dq076/data/cases/site/'\n",
    "    data_path = f'{cases_path}/{forcing}/{mode}/'\n",
    "\n",
    "    stnlist = f\"/share/home/dq076/data/ME/FLUXNET-CH4/FLX_AA-Flx_CH4-META_20201112135337801132.csv\"\n",
    "    station_lists = pd.read_csv(stnlist, header=0)\n",
    "    station_lists = station_lists[station_lists['FLUXNET-CH4_DATA_POLICY'] == 'CCBY4.0'].reset_index(drop=True)\n",
    "    \n",
    "    results = Parallel(n_jobs=24)(delayed(merge)(i,station_lists,data_path) for i in range(station_lists.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4998acdd",
   "metadata": {},
   "source": [
    "## 4.Draw the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85137c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import geopandas as gpd\n",
    "from pylab import rcParams\n",
    "import xarray as xr\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.font_manager as fm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "from matplotlib.dates import MonthLocator, DateFormatter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "stnlist = f\"/share/home/dq076/data/ME/FLUXNET-CH4/FLX_AA-Flx_CH4-META_20201112135337801132.csv\"\n",
    "station_lists = pd.read_csv(stnlist, header=0)\n",
    "station_lists = station_lists[station_lists['FLUXNET-CH4_DATA_POLICY'] == 'CCBY4.0'].reset_index(drop=True)\n",
    "# mode = 'no_spin_up'\n",
    "mode = 'spin_up_80yr'\n",
    "forcing = 'CRUJRA'\n",
    "# siteid = 'BW-Gum' \n",
    "# yrstt = 2018\n",
    "# yrend = 2018\n",
    "\n",
    "for i in range(station_lists.shape[0]):\n",
    "    try:\n",
    "        station_list = station_lists.iloc[i]\n",
    "        siteid = station_list['SITE_ID']\n",
    "        yrstt = station_list['YEAR_START']\n",
    "        yrend = station_list['YEAR_END']\n",
    "        pfttyp = station_list['SITE_CLASSIFICATION']\n",
    "        if pfttyp == 'Upland' or pfttyp == 'Drained' and isinstance(station_list['UPLAND_CLASS'], str):\n",
    "            pfttyp = station_list['UPLAND_CLASS']\n",
    "\n",
    "        var = 'ch4_surf_flux_tot'\n",
    "        var1 = {'ch4_surf_flux_tot':'f_ch4_surf_flux_tot'}\n",
    "        trans = {'ch4_surf_flux_tot':1/16e6}\n",
    "        var2 = {'ch4_surf_flux_tot':'FCH4_F'}\n",
    "        title = {'ch4_surf_flux_tot':'CH4 Surface Flux'}\n",
    "        ytitle = title\n",
    "        unit = {'ch4_surf_flux_tot':'nmol CH4 m-2 s-1'}\n",
    "        path1 = f'/share/home/dq076/data/cases/site/{forcing}/{mode}/{siteid}/postdata/'\n",
    "        path2 = f'/share/home/dq076/data/ME/FLUXNET-CH4/FLX_{siteid}_FLUXNET-CH4_{yrstt}-{yrend}_1-1/'\n",
    "\n",
    "        label = ['CoLM','FLUXNET-CH4']\n",
    "        color = [\"#69aa4c\",\"#dd5e2d\"]\n",
    "\n",
    "        data1 = xr.open_dataset(f'{path1}{siteid}_hist_{yrstt}-{yrend}.nc')\n",
    "        data2 = xr.open_dataset(f'{path2}{siteid}_FLUXNET-CH4_DD.nc')\n",
    "        # print(data1)\n",
    "        # print(data2)\n",
    "        data1 = data1[var1[var]][:,0]*6.25e7\n",
    "        data2 = data2[var2[var]][:,0,0]\n",
    "        x = data2['time']\n",
    "        print(title[var])\n",
    "\n",
    "        # 检查 data1 是否全为 0 或 NaN\n",
    "        data1_values = data1.values\n",
    "        if np.all((np.isnan(data1_values) | (data1_values == 0))):\n",
    "            print(f\"站点 {siteid} 异常: data1 全为 0 或 NaN, 跳过绘制\")\n",
    "            continue\n",
    "\n",
    "        fig = plt.figure(figsize=(8, 4), dpi=300)\n",
    "        fig.subplots_adjust(left=0.05, right=0.98, \n",
    "                        bottom=0.14, top=0.95, hspace=0.8) \n",
    "        gs = GridSpec(2, 48)\n",
    "        ax = fig.add_subplot(gs[:, :])\n",
    "        ax.grid(ls = \"--\", lw = 0.25, color = \"#4E616C\")\n",
    "        ax.plot(data1['time'], data1, mfc = \"white\",lw = 1, ms = 2, color = color[0], label=label[0])\n",
    "        ax.plot(data2['time'], data2, mfc = \"white\",lw = 1, ms = 2, color = color[1], label=label[1])\n",
    "        # ax.set_title(f\"{siteid} {mode}  {title[var]} ({unit[var]})\")\n",
    "        ax.set_title(f\"{siteid} {pfttyp} ({mode})\")\n",
    "\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel(f\"{ytitle[var]} ({unit[var]})\")\n",
    "\n",
    "        ax.xaxis.set_major_locator(MonthLocator(interval=6))\n",
    "        ax.xaxis.set_major_formatter(DateFormatter('%Y-%m'))\n",
    "        print(x[0])\n",
    "        ax.set_xlim(x[0],x[-1])\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as exc:\n",
    "            print(f\"执行异常: {exc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
